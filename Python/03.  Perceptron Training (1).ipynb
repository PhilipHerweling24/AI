{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mplot\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single perceptron with n input neurons, n weigths and a bias\n",
    "# To keep things simple as there is only 1 perceptron, we store weights as a\n",
    "# Python vector of shape (n,) rather Nielsen's way as shape (1,n), a single row matrix.\n",
    "class Perceptron(object):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        np.random.seed(2019)\n",
    "        self.w = np.random.randn(n,)  # vector of random weights from Gaussian or Normal distribution\n",
    "        self.b = np.random.randn()  # single randon value\n",
    "    \n",
    "    # feedforward propagates each input vector x in xs thru the network (only 1 neuron here)\n",
    "    # and returns an array (list) of outputs, one for each input vector. \n",
    "    # Note that in this Perceptron we are using the step() activation function, not sigmoid()\n",
    "    def feedforward(self, xs):\n",
    "        res = [self.w.dot(x) + self.b for x in xs]\n",
    "        return np.array([step(r) for r in res])\n",
    "   \n",
    "    # the following operation illustrates the training algorithm for a Perceptron\n",
    "    def train(self, xs, ys, epochs, eta):\n",
    "        # we use cost to record the cost or loss in the Perceptron's current performance\n",
    "        # over all the training input vectors for the current epoch or training iteration.\n",
    "        # This is not needed for training, but we record it to plot training progress later on.\n",
    "        \n",
    "        cost = np.zeros((epochs,), dtype=float)\n",
    "        \n",
    "        for ep in range(epochs):                \n",
    "            del_w = np.zeros((self.n,), dtype=float)\n",
    "            del_b = 0.0\n",
    "            \n",
    "            for x,y in zip(xs, ys):  \n",
    "                # x is usually an input vector, xs an array of vectors, y is a single value\n",
    "                # first compute weighted sum z, neuron's actual output a, and output error e\n",
    "                z = self.w.dot(x) + self.b\n",
    "                a = step(z)\n",
    "                e = y - a     # error e = desired or target ouput less the actual output\n",
    "                \n",
    "                # compute the squared cost (loss) for each input and add them to compute \n",
    "                # cost for an entire training set. \n",
    "                \n",
    "                cost[ep] += 0.5 * e**2\n",
    "                \n",
    "                # print(z, a, e, cost[ep])\n",
    "                # learning formula for single perceptron with step activation\n",
    "                # note del_w and x are vectors, so all the weight adjustments \n",
    "                # for a single input are computed in 1 go here\n",
    "                \n",
    "                del_w += eta * e * x  \n",
    "                del_b += eta * e      \n",
    "                                                                \n",
    "            \n",
    "            # Strictly should divide the sum by len(xs) to get average cost per training input\n",
    "            # update the weigths and bias with cumulative update based on all the training data\n",
    "            # Note that in the spreadshhet example the weights are updated after each training\n",
    "            # input whereas here we update them after each epoch.\n",
    "            # cost[e] = cost[e] / len(xs)\n",
    "            \n",
    "            self.w += del_w\n",
    "            self.b += del_b\n",
    "            \n",
    "            # print(cost[ep])\n",
    "        return cost\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step activation function        \n",
    "def step(weighted_sum):\n",
    "    if weighted_sum <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "# sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training inputs and desired outputs for AND gate\n",
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "targets_AND = np.array([0, 0, 0, 1])\n",
    "# create a new perceptron object\n",
    "p = Perceptron(2)\n",
    "\n",
    "# see what is initial output is for the 4 possible 2-bit inputs\n",
    "print(p.feedforward(inputs))\n",
    "\n",
    "# train p to behave like AND gate\n",
    "epochs = 30\n",
    "cst = p.train(inputs, targets_AND, epochs, 0.1)\n",
    "\n",
    "# check output again after training\n",
    "print(p.feedforward(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = [e for e in range(epochs)]\n",
    "plt.plot(eps, cst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eps[3:12], cst[3:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with increased learning constant eta = 0.3\n",
    "p2 = Perceptron(2)\n",
    "epochs = 30\n",
    "cst2 = p2.train(inputs, targets_AND, epochs, 0.3)\n",
    "eps = [e for e in range(epochs)]\n",
    "plt.plot(eps, cst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with increased learning constant eta = 3.0\n",
    "p3 = Perceptron(2)\n",
    "epochs = 30\n",
    "cst3 = p3.train(inputs, targets_AND, epochs, 3.0)\n",
    "eps = [e for e in range(epochs)]\n",
    "plt.plot(eps, cst3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:  train another perceptron object to act as an OR gate and and try to train a third \n",
    "# one as an XOR gate. Also experiment with the learning constant eta, trying values 1.0, 0,3 etc. \n",
    "\n",
    "\n",
    "# Exercise: Create a class for SigmoidPerceptron which is nearly identical to class Perceptron\n",
    "# except that it uses sigmoid(z) as the activation function and you will need to modify the learning\n",
    "# formula from  del_w = eta * e * x  where e = (y-a) to\n",
    "# del_w = -eta * delta * x  where  delta = (a-y) * a * (1-a)  \n",
    "# Note that a * (1-a) is the derivative of sigmoid, where a = sigmoid(z) and z = w.dot(x) + b\n",
    "\n",
    "# Then try to train a SigmoidPerceptron for AND, OR. Will likely need many more epochs.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
